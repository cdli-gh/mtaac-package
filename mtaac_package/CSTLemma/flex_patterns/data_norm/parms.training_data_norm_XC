               ; verbose (-v <n>: yes (n=1 high priority n > 1 lower priority) -v-: no)
-v -
               ; keep intermediary files (-x: yes -x-: no)
-x -
               ; 10-fold cross validation (-VX: yes, overrules T and t options ;-VX: no)
;-VX
               ; External training program
;-G<program name>
               ; External lemmatizer
;-E<program name>
               ; flex rules (input file, binary format)
;-b (not specified, not doing actions with already created flex rules.)
               ; Word list (Optional if -b is specified. Otherwise N/A) (-I filename)
;-I  (N/A)
               ; Output, lemmas of words in input (-I option)
;-O  (N/A)
               ; word/lemma list
-i C:\Users\Ilya\Desktop\MTAAC\test 2\test files\data\training_data_norm
               ; extra file name affix
-e 
               ; suffix only (-s: yes -s-: no)
-s -
               ; make rules with infixes less prevalent(-A: yes -A-: no)
-A -
               ; columns (1 or F or W=word,2 or B or L=lemma,3 or T=tags,0 or O=other)
-n FL
               ; specific POS tag (default: empty string)
-k (null)
               ; max recursion depth when attempting to create candidate rule
-Q 1
               ; flex rules (output, binary format, can be left unspecified)
;-o (Not specified, autogenerated)
               ; temp dir
-j tmp
               ; penalties to decide which rule survives (4 or 6 floating point numbers: R=>R;W=>R;R=>W;W=>W[;R=>N/A;W=>NA], where R=#right cases, W=#wrong cases, N/A=#not applicable cases, previous success state=>success state after rule application)
-D -0.0010477632;-0.6913966894;0.7130505762;-0.0087601821;-0.0913392680;-0.0714758225;
               ; compute parms (-p: yes -p-: no)
-p 
               ; expected optimal pruning threshold (only effective in combination with -XW)
-C -1
               ; tree penalty (-XC: constant -XD: more support is better -XE: higher entropy is better -XW: Fewer pattern characters other than wildcards is better)
-X C
               ; current parameters (-P filename)
-P parms.txt
               ; best parameters (-B filename)
-B best.txt
               ; start training with minimal fraction of training pairs (-Ln: 0.0 < n <= 1.0)
-L 1.000000
               ; end training with maximal fraction of training pairs (-Hn: 0.0 < n <= 1.0)
-H 1.000000
               ; number of differently sized fractions of trainingdata (natural number)
-K 20
               ; number of iterations of training with same fraction of training data when fraction is minimal (positive number)
-N 100.000000
               ; number of iterations of training with same fraction of training data when fraction is maximal (positive number)
-M 10.000000
               ; competition function (deprecated)
;-f  (N/A)
               ; redo training after homographs for next round are removed (-R: yes -R-: no)
;-R - (N/A)
               ; max. pruning threshold to evaluate
-c 5
               ; test with the training data (-T: yes -T-: no)
-T 
               ; test with data not used for training (-t: yes -t-: no)
-t 
               ; create flexrules using full training set (-F: yes -F-: no)
-F 
               ; Number of clusters found in word/lemma list: 1
               ; Number of lines found in word/lemma list:    9912

; Evaluation:
; -----------
; Lemmatization results for all data in the training set.
; For pruning threshold 0 there may be no errors (diff%).
; (This is not necessarily true for external trainers/lemmatizers that
; e.g. do not return all possible solutions in case of ambiguity.)

; prun. thrshld.              0              1              2              3              4              5 
; rules             4946.000000    2217.000000     474.000000     280.000000     208.000000     167.000000 
; rules%              51.376337      23.028981       4.923652       2.908487       2.160590       1.734704 
; same%               96.644853      70.208788      54.731484      51.262075      49.537758      47.803054 
; ambi1%               1.641217       3.718708       2.222915       1.848966       1.578893       1.672380 
; ambi2%               1.641217       3.791420       2.139815       1.610055       1.059520       1.132232 
; ambi3%               0.072712       0.083100       0.072712       0.051937       0.000000       0.000000 
; diff%                0.000000      22.197985      40.833074      45.226966      47.823829      49.392334 
; same%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi1%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi2%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi3%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; 
;Evaluation of prediction of ambiguity (whether a word has more than one possible lemma)
;---------------------------------------------------------------------------------------
; amb.rules%           3.355147       9.462969       8.174925       7.302379       6.263634       6.751844 
; false_amb%           0.000000       7.406253       7.416641       6.814168       5.972785       6.419445 
; false_not_amb%       0.000000       1.298431       2.596863       2.866937       3.064298       3.022749 
; true_amb%            3.355147       2.056715       0.758284       0.488210       0.290849       0.332398 
; true_not_amb%       96.644853      89.238600      89.228212      89.830685      90.672068      90.225408 
; precision            1.000000       0.121921       0.048634       0.034584       0.023769       0.025237 
; recall               1.000000       0.613003       0.226006       0.145511       0.086687       0.099071 

; Evaluation:
; -----------
; Lemmatization results for data that is not part of the training data.

; prun. thrshld.              0              1              2              3              4              5 
; rules             4867.000000    2158.000000     479.000000     279.000000     209.000000     171.000000 
; rules%              51.296374      22.744519       5.048482       2.940556       2.202782       1.802277 
; same%               48.201439      53.237410      49.640288      48.201439      46.762590      43.884892 
; ambi1%               2.158273       2.877698       1.438849       0.719424       0.719424       2.158273 
; ambi2%               1.438849       1.438849       2.158273       2.877698       2.158273       2.158273 
; ambi3%               0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%               48.201439      42.446043      46.762590      48.201439      50.359712      51.798561 
; same%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi1%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi2%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi3%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; 
;Evaluation of prediction of ambiguity (whether a word has more than one possible lemma)
;---------------------------------------------------------------------------------------
; amb.rules%           5.755396       7.913669       5.035971       3.597122       3.597122       7.194245 
; false_amb%           5.755396       7.194245       5.035971       3.597122       3.597122       7.194245 
; false_not_amb%       6.474820       5.755396       6.474820       6.474820       6.474820       6.474820 
; true_amb%            0.000000       0.719424       0.000000       0.000000       0.000000       0.000000 
; true_not_amb%       87.769784      86.330935      88.489209      89.928058      89.928058      86.330935 
; precision            0.000000       0.047619       0.000000       0.000000       0.000000       0.000000 
; recall               0.000000       0.111111       0.000000       0.000000       0.000000       0.000000 
; 
; Power law relating the number of rules in the decision tree to the number of examples in the training data
;----------------------------------------------------------------------------------------------------------
; #rules =        0.995*N^0.927  0.521*N^0.910  0.118*N^0.904  0.027*N^1.022  0.014*N^1.065  0.008*N^1.115 

; Postscriptum

; The number of rules can be estimated from the number of training examples by
; a power law. See the last line in the table above, which is based on 7
; different samples from the total available training data mass varying in size
; from 1.54 % to 98.56 %