               ; verbose (-v <n>: yes (n=1 high priority n > 1 lower priority) -v-: no)
-v -
               ; keep intermediary files (-x: yes -x-: no)
-x -
               ; 10-fold cross validation (-VX: yes, overrules T and t options ;-VX: no)
;-VX
               ; External training program
;-G<program name>
               ; External lemmatizer
;-E<program name>
               ; flex rules (input file, binary format)
;-b (not specified, not doing actions with already created flex rules.)
               ; Word list (Optional if -b is specified. Otherwise N/A) (-I filename)
;-I  (N/A)
               ; Output, lemmas of words in input (-I option)
;-O  (N/A)
               ; word/lemma list
-i C:\Users\Ilya\Desktop\MTAAC\test 2\test files\data\training_data_norm_u
               ; extra file name affix
-e 
               ; suffix only (-s: yes -s-: no)
-s -
               ; make rules with infixes less prevalent(-A: yes -A-: no)
-A -
               ; columns (1 or F or W=word,2 or B or L=lemma,3 or T=tags,0 or O=other)
-n FL
               ; specific POS tag (default: empty string)
-k (null)
               ; max recursion depth when attempting to create candidate rule
-Q 1
               ; flex rules (output, binary format, can be left unspecified)
;-o (Not specified, autogenerated)
               ; temp dir
-j tmp
               ; penalties to decide which rule survives (4 or 6 floating point numbers: R=>R;W=>R;R=>W;W=>W[;R=>N/A;W=>NA], where R=#right cases, W=#wrong cases, N/A=#not applicable cases, previous success state=>success state after rule application)
-D -0.0002860953;-0.6876118678;0.7111971374;-0.0027739392;0.0253876118;-0.1440008458;
               ; compute parms (-p: yes -p-: no)
-p 
               ; expected optimal pruning threshold (only effective in combination with -XW)
-C -1
               ; tree penalty (-XC: constant -XD: more support is better -XE: higher entropy is better -XW: Fewer pattern characters other than wildcards is better)
-X C
               ; current parameters (-P filename)
-P parms.txt
               ; best parameters (-B filename)
-B best.txt
               ; start training with minimal fraction of training pairs (-Ln: 0.0 < n <= 1.0)
-L 1.000000
               ; end training with maximal fraction of training pairs (-Hn: 0.0 < n <= 1.0)
-H 1.000000
               ; number of differently sized fractions of trainingdata (natural number)
-K 20
               ; number of iterations of training with same fraction of training data when fraction is minimal (positive number)
-N 100.000000
               ; number of iterations of training with same fraction of training data when fraction is maximal (positive number)
-M 10.000000
               ; competition function (deprecated)
;-f  (N/A)
               ; redo training after homographs for next round are removed (-R: yes -R-: no)
;-R - (N/A)
               ; max. pruning threshold to evaluate
-c 5
               ; test with the training data (-T: yes -T-: no)
-T 
               ; test with data not used for training (-t: yes -t-: no)
-t 
               ; create flexrules using full training set (-F: yes -F-: no)
-F 
               ; Number of clusters found in word/lemma list: 1
               ; Number of lines found in word/lemma list:    9912

; Evaluation:
; -----------
; Lemmatization results for all data in the training set.
; For pruning threshold 0 there may be no errors (diff%).
; (This is not necessarily true for external trainers/lemmatizers that
; e.g. do not return all possible solutions in case of ambiguity.)

; prun. thrshld.              0              1              2              3              4              5 
; rules             5218.000000    2246.000000     483.000000     311.000000     231.000000     186.000000 
; rules%              54.128631      23.298755       5.010373       3.226141       2.396266       1.929461 
; same%               96.981328      69.802905      53.952282      50.601660      48.226141      46.379668 
; ambi1%               1.483402       3.941909       2.676349       2.085062       2.136929       2.188797 
; ambi2%               1.483402       3.941909       2.251037       1.680498       1.773859       1.763485 
; ambi3%               0.051867       0.124481       0.031120       0.020747       0.051867       0.072614 
; diff%                0.000000      22.188797      41.089212      45.612033      47.811203      49.595436 
; same%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi1%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi2%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi3%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; 
;Evaluation of prediction of ambiguity (whether a word has more than one possible lemma)
;---------------------------------------------------------------------------------------
; amb.rules%           3.018672      10.363071       9.699170       8.454357       8.941909       9.605809 
; false_amb%           0.000000       8.516598       9.004149       8.018672       8.620332       9.284232 
; false_not_amb%       0.000000       1.172199       2.323651       2.582988       2.697095       2.697095 
; true_amb%            3.018672       1.846473       0.695021       0.435685       0.321577       0.321577 
; true_not_amb%       96.981328      88.464730      87.977178      88.962656      88.360996      87.697095 
; precision            1.000000       0.097802       0.037160       0.026448       0.018311       0.017024 
; recall               1.000000       0.611684       0.230241       0.144330       0.106529       0.106529 

; Evaluation:
; -----------
; Lemmatization results for data that is not part of the training data.

; prun. thrshld.              0              1              2              3              4              5 
; rules             6867.000000    2199.000000     476.000000     305.000000     228.000000     188.000000 
; rules%              72.276602      23.144932       5.009999       3.210188       2.399747       1.978739 
; same%               47.482014      45.323741      44.604317      43.884892      43.884892      42.446043 
; ambi1%               2.158273       1.438849       0.719424       0.719424       1.438849       2.158273 
; ambi2%               0.719424       2.877698       2.877698       0.719424       0.719424       0.719424 
; ambi3%               0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%               49.640288      50.359712      51.798561      54.676259      53.956835      54.676259 
; same%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi1%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi2%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; ambi3%stdev          0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; diff%stdev           0.000000       0.000000       0.000000       0.000000       0.000000       0.000000 
; 
;Evaluation of prediction of ambiguity (whether a word has more than one possible lemma)
;---------------------------------------------------------------------------------------
; amb.rules%           6.474820       9.352518       7.913669       6.474820       7.913669      10.071942 
; false_amb%           6.474820       8.633094       6.474820       5.755396       7.194245       9.352518 
; false_not_amb%       5.035971       4.316547       3.597122       4.316547       4.316547       4.316547 
; true_amb%            0.000000       0.719424       1.438849       0.719424       0.719424       0.719424 
; true_not_amb%       88.489209      86.330935      88.489209      89.208633      87.769784      85.611511 
; precision            0.000000       0.040000       0.100000       0.058824       0.047619       0.037037 
; recall               0.000000       0.142857       0.285714       0.142857       0.142857       0.142857 
; 
; Power law relating the number of rules in the decision tree to the number of examples in the training data
;----------------------------------------------------------------------------------------------------------
; #rules =        0.831*N^0.962  0.500*N^0.915  0.080*N^0.950  0.017*N^1.086  0.017*N^1.047  0.007*N^1.127 

; Postscriptum

; The number of rules can be estimated from the number of training examples by
; a power law. See the last line in the table above, which is based on 7
; different samples from the total available training data mass varying in size
; from 1.54 % to 98.56 %